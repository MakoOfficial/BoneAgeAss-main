{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from albumentations.augmentations.transforms import Lambda, RandomBrightnessContrast\n",
    "from albumentations.augmentations.geometric.transforms import ShiftScaleRotate, HorizontalFlip\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.crops.transforms import RandomResizedCrop\n",
    "from albumentations import Compose\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import mymodel\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import string\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(os.path.join(data_dir, 'boneage-training-dataset.csv'))\n",
    "# torch.tensor(data['id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyfile(fname, target_dir):\n",
    "    \"\"\"将文件复制到指定文件夹\"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    shutil.copy(fname, target_dir)\n",
    "\n",
    "\n",
    "def read_labels(data_dir, fname):\n",
    "    \"\"\"读取标签，并将其标准化\"\"\"\n",
    "    data = pd.read_csv(os.path.join(data_dir, fname))\n",
    "    data = data.sort_values(by=\"id\", ascending=True)\n",
    "    data['male'] = data['male'].astype('float32')\n",
    "    # data['boneage'] = data['boneage'].astype('float32')\n",
    "    data = np.array(data)\n",
    "    id = data[:, :1]\n",
    "    age = data[:, 1:2]\n",
    "    gender = data[:, 2:]\n",
    "    return torch.tensor(id), torch.tensor(age), torch.tensor(gender)\n",
    "\n",
    "# read_labels(data_dir, 'boneage-training-dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d2l里的获取标签\n",
    "def read_csv_labels_noMale(fname):\n",
    "    \"\"\"读取标签，返回字典格式`\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # Skip the file header line (column name) 跳过文件头\n",
    "        lines = f.readlines()[1:]\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    # return dict(((id, boneage) for id, boneage, male in tokens))\n",
    "    return dict((id, [boneage, male]) for id, boneage, male in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'180': 418,\n",
       "         '12': 11,\n",
       "         '94': 492,\n",
       "         '120': 992,\n",
       "         '82': 385,\n",
       "         '138': 529,\n",
       "         '150': 678,\n",
       "         '156': 1113,\n",
       "         '36': 106,\n",
       "         '126': 198,\n",
       "         '132': 1084,\n",
       "         '57': 18,\n",
       "         '188': 1,\n",
       "         '4': 1,\n",
       "         '24': 77,\n",
       "         '60': 278,\n",
       "         '159': 69,\n",
       "         '106': 478,\n",
       "         '30': 38,\n",
       "         '149': 2,\n",
       "         '33': 13,\n",
       "         '78': 55,\n",
       "         '88': 55,\n",
       "         '162': 682,\n",
       "         '32': 24,\n",
       "         '54': 89,\n",
       "         '174': 97,\n",
       "         '144': 657,\n",
       "         '27': 9,\n",
       "         '170': 3,\n",
       "         '113': 48,\n",
       "         '108': 312,\n",
       "         '136': 6,\n",
       "         '165': 64,\n",
       "         '42': 89,\n",
       "         '21': 15,\n",
       "         '69': 193,\n",
       "         '50': 95,\n",
       "         '90': 49,\n",
       "         '192': 172,\n",
       "         '84': 274,\n",
       "         '96': 302,\n",
       "         '216': 72,\n",
       "         '140': 4,\n",
       "         '100': 60,\n",
       "         '72': 254,\n",
       "         '168': 892,\n",
       "         '48': 71,\n",
       "         '139': 3,\n",
       "         '186': 138,\n",
       "         '13': 2,\n",
       "         '135': 27,\n",
       "         '146': 3,\n",
       "         '189': 15,\n",
       "         '51': 11,\n",
       "         '102': 46,\n",
       "         '134': 3,\n",
       "         '39': 16,\n",
       "         '64': 7,\n",
       "         '104': 3,\n",
       "         '129': 1,\n",
       "         '147': 8,\n",
       "         '14': 1,\n",
       "         '45': 10,\n",
       "         '28': 10,\n",
       "         '128': 3,\n",
       "         '18': 27,\n",
       "         '86': 1,\n",
       "         '153': 42,\n",
       "         '110': 1,\n",
       "         '210': 12,\n",
       "         '55': 12,\n",
       "         '222': 2,\n",
       "         '66': 39,\n",
       "         '198': 31,\n",
       "         '200': 2,\n",
       "         '109': 1,\n",
       "         '80': 1,\n",
       "         '9': 5,\n",
       "         '114': 63,\n",
       "         '112': 8,\n",
       "         '204': 200,\n",
       "         '148': 1,\n",
       "         '154': 3,\n",
       "         '58': 4,\n",
       "         '67': 3,\n",
       "         '116': 1,\n",
       "         '118': 2,\n",
       "         '101': 4,\n",
       "         '111': 2,\n",
       "         '124': 4,\n",
       "         '56': 2,\n",
       "         '130': 2,\n",
       "         '158': 4,\n",
       "         '228': 19,\n",
       "         '123': 2,\n",
       "         '38': 1,\n",
       "         '29': 1,\n",
       "         '164': 6,\n",
       "         '137': 4,\n",
       "         '117': 1,\n",
       "         '15': 16,\n",
       "         '172': 1,\n",
       "         '184': 3,\n",
       "         '143': 1,\n",
       "         '76': 25,\n",
       "         '169': 2,\n",
       "         '121': 2,\n",
       "         '214': 1,\n",
       "         '115': 5,\n",
       "         '75': 15,\n",
       "         '166': 2,\n",
       "         '17': 2,\n",
       "         '34': 3,\n",
       "         '151': 1,\n",
       "         '163': 2,\n",
       "         '194': 1,\n",
       "         '177': 1,\n",
       "         '125': 6,\n",
       "         '74': 1,\n",
       "         '81': 2,\n",
       "         '91': 2,\n",
       "         '87': 3,\n",
       "         '105': 1,\n",
       "         '183': 5,\n",
       "         '152': 5,\n",
       "         '16': 3,\n",
       "         '176': 3,\n",
       "         '133': 1,\n",
       "         '141': 2,\n",
       "         '173': 1,\n",
       "         '196': 2,\n",
       "         '160': 3,\n",
       "         '197': 1,\n",
       "         '171': 1,\n",
       "         '167': 1,\n",
       "         '20': 1,\n",
       "         '62': 3,\n",
       "         '212': 2,\n",
       "         '1': 1,\n",
       "         '49': 1,\n",
       "         '10': 4,\n",
       "         '63': 2,\n",
       "         '52': 1,\n",
       "         '182': 1,\n",
       "         '142': 1,\n",
       "         '179': 1,\n",
       "         '6': 2,\n",
       "         '70': 4,\n",
       "         '93': 1,\n",
       "         '46': 5,\n",
       "         '107': 1,\n",
       "         '40': 2,\n",
       "         '206': 1,\n",
       "         '65': 2,\n",
       "         '37': 1,\n",
       "         '161': 1,\n",
       "         '103': 1,\n",
       "         '77': 1,\n",
       "         '43': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = read_csv_labels_noMale(os.path.join(data_dir, 'boneage-training-dataset.csv'))\n",
    "labels['2277']\n",
    "# labels = pd.read_csv(os.path.join(data_dir, 'boneage-training-dataset.csv'))\n",
    "# row = labels.iloc[1]\n",
    "# str(row['id'])\n",
    "# filename = os.path.join(data_dir, 'boneage-valid-dataset', f\"{row['id']}.png\")\n",
    "# filename\n",
    "# type(row['boneage'])\n",
    "# # len(labels)\n",
    "# # for i in labels.values():\n",
    "# #     print(i)\n",
    "# # labels.get('1377', 0)\n",
    "# collections.Counter(labels.values())\n",
    "age_list = list(np.array(list(labels.values()))[:, 0])\n",
    "class_count = collections.Counter(age_list)\n",
    "class_count\n",
    "# type(labels.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_dir, labels, target_size):\n",
    "    class_count = collections.Counter(labels.values())\n",
    "    myBatch = {}\n",
    "    lengthOfDataset = len(labels)\n",
    "    for key in class_count.keys():\n",
    "        myBatch[key] = math.ceil(target_size * class_count[key] / lengthOfDataset)\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'boneage-training-dataset')):\n",
    "        # 获取该文件的标签\n",
    "        label = labels[train_file.split('.')[0]]\n",
    "        # 获取文件的路径\n",
    "        fname = os.path.join(data_dir, 'boneage-training-dataset', train_file)\n",
    "        # 复制该文件到目标文件夹\n",
    "        copyfile(fname, os.path.join(data_dir, 'train_valid_test', \n",
    "                                    'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < myBatch[label]:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test', \n",
    "                                        'valid', label))\n",
    "            # 将该标签加入记录， 若count中没有则默认返回默认值0\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                        'train', label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先要获得每个年龄的数据集在总数据集中的占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_labels = np.array(labels)\n",
    "# type(n['180'])\n",
    "# class_count = collections.Counter(labels.values())\n",
    "# myBatch = {}\n",
    "# lengthOfDataset = len(labels)\n",
    "# target_size = 2000\n",
    "# count = 0\n",
    "# for key in class_count.keys():\n",
    "#     myBatch[key] = math.ceil(target_size * class_count[key] / lengthOfDataset)\n",
    "#     count += myBatch[key]\n",
    "# myBatch\n",
    "# print(new_labels)\n",
    "# data = np.array(data)\n",
    "# age = data[:, 1:2]\n",
    "# age = torch.tensor(age)\n",
    "# for i in range(age.shape[0]):\n",
    "#     record[age[i]] += 1\n",
    "# age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatSmallDataset(data_dir, labels, target_size, DF):\n",
    "    \"\"\"创建新的数据集\"\"\"\n",
    "    train_list = [['id', 'boneage', 'male']]\n",
    "    age_list = list(np.array(list(labels.values()))[:, 0])\n",
    "    class_count = collections.Counter(age_list)\n",
    "    myBatch = {}\n",
    "    lengthOfDataset = len(labels)\n",
    "    for key in class_count.keys():\n",
    "        myBatch[key] = math.ceil(target_size * class_count[key] / lengthOfDataset)\n",
    "    label_count = {}\n",
    "    for idx, row in DF.iterrows():\n",
    "        # 获取当前文件路径\n",
    "        filename = os.path.join(data_dir, 'boneage-training-dataset', f\"{row['id']}.png\")\n",
    "        # 获取该文件的标签\n",
    "        label = str(row['boneage'])\n",
    "        if label not in label_count or label_count[label] < myBatch[label]:\n",
    "            copyfile(filename, os.path.join(data_dir, 'small-dataset', label))\n",
    "            train_list.append([row['id'], row['boneage'], row['male']])\n",
    "            # 将该标签加入记录， 若count中没有则默认返回默认值0\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "    with open('./dataset/small-dataset.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in train_list:\n",
    "            writer.writerow(row)\n",
    "    # for train_file in os.listdir(os.path.join(data_dir, 'boneage-training-dataset')):\n",
    "    #     # 获取改文件的id,type:str\n",
    "    #     fID = train_file.split('.')[0]\n",
    "    #     # 获取该文件的标签\n",
    "    #     label = labels[fID]\n",
    "    #     # 获取文件的路径\n",
    "    #     fname = os.path.join(data_dir, 'boneage-training-dataset', train_file)\n",
    "    #     # 复制该文件到目标文件夹\n",
    "    #     if label not in label_count or label_count[label] < myBatch[label]:\n",
    "    #         copyfile(fname, os.path.join(data_dir, 'boneage-small-dataset'))\n",
    "    #         # 获取该图片的所有label\n",
    "    #         row = DF.iloc[]\n",
    "    #         valid_list.append([DF])\n",
    "\n",
    "    #         # 将该标签加入记录， 若count中没有则默认返回默认值0\n",
    "    #         label_count[label] = label_count.get(label, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_data(data_dir, target_size):\n",
    "    labels = read_csv_labels_noMale(os.path.join(data_dir, 'boneage-training-dataset.csv'))\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'boneage-training-dataset.csv'))\n",
    "    creatSmallDataset(data_dir, labels, target_size, df)\n",
    "    # d2l.reorg_test(data_dir)\n",
    "\n",
    "\n",
    "# batch_size = 32\n",
    "# valid_ratio = 0.1\n",
    "target_size = 1000\n",
    "# reorg_data(data_dir, target_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已获得原数据集分布的小数据集，接下来将图片数小于10的图片增广到10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(input_root, aug, aug_size, label_list):\n",
    "    \"\"\"对文件夹下的所有图片进行增广\"\"\"\n",
    "    labels = [] # str的列表\n",
    "    for root, dirs, files in os.walk(input_root):\n",
    "        dirs_size = len(dirs)\n",
    "        files_size = len(files)\n",
    "        if(dirs_size > 0):# 第一层，没有文件，只获取标签\n",
    "            labels = dirs\n",
    "        elif(files_size < aug_size):\n",
    "            need_size = aug_size - files_size\n",
    "            files = sorted(files)\n",
    "            for i in need_size:\n",
    "                idx = i%files_size\n",
    "                fname = files[idx]\n",
    "                label = label_list[fname.split('.')[0]]\n",
    "                \n",
    "                \n",
    "                    \n",
    "        # print(f'root:{root}, dirs:{dirs}, files:{files}')\n",
    "    # Y = [aug(img) for _ in range(num_rows * num_cols)]\n",
    "    # d2l.show_images(Y, num_rows, num_cols, scale=scale)\n",
    "apply('./dataset/small-dataset/', 0, 0, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
