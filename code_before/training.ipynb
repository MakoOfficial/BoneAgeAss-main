{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：整理数据集\n",
    "首先一个问题就是如何生成验证集，文中的数据集有验证集，而我们下载的是没用的，所以需要自己去生成\n",
    "不过还是先从读取图片和csv文件开始吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import collections\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "import csv\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from d2l import torch as d2l\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from mymodel import MMANet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些变量\n",
    "#文件路径\n",
    "data_dir = './dataset/'\n",
    "batch_size = 4\n",
    "valid_ratio = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在读取文件时，还有一个任务就是修改图片尺寸，使其成为512x512的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iamge(data_dir, fname, image_size=512):\n",
    "    img = Image.open(os.path.join(data_dir, fname))\n",
    "    # 开始修改尺寸\n",
    "    w, h = img.size\n",
    "    long = max(w, h)\n",
    "    # 按比例缩放成512\n",
    "    w, h = int(w/long*image_size), int(h/long*image_size)\n",
    "    # 压缩并插值\n",
    "    img = img.resize((w, h), Image.ANTIALIAS)\n",
    "    # 然后是给短边扩充，使用ImageOps.expand\n",
    "    delta_w, delta_h = image_size - w, image_size - h\n",
    "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "    # 转化成np.array再返回\n",
    "    return np.array(ImageOps.expand(img, padding).convert(\"RGB\"))\n",
    "\n",
    "training_dir = './dataset/boneage-training-dataset/'\n",
    "this_pic = '1377.png'\n",
    "convert_pic = read_iamge(training_dir, this_pic)\n",
    "convert_pic.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将图片修改成了(512, 512, 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv文件的读取采用pandas.readcsv,读出的是DataFrame格式，用df标记,将id和age提取出来当做dict处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_csv = pd.read_csv(os.path.join(data_dir, 'boneage-training-dataset.csv'))\n",
    "# training_dict = dict(training_csv['BoneAge'])\n",
    "# training_dict = {}\n",
    "# for index, item in list(training_csv['ID']), list(training_csv['BoneAge']):\n",
    "    # training_dict[index] = item\n",
    "# training_dict.values()\n",
    "# collections.Counter(training_dict.values()).most_common()[-1][1]\n",
    "# list(training_csv['ID'])\n",
    "# training_dict = zip(list(training_csv['ID']), list(training_csv['BoneAge']))\n",
    "# training_dict = dict(zip(list(training_csv['ID']), list(training_csv['BoneAge'])))\n",
    "len(training_csv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何将验证集从原始的训练集中拆分出来。首先定义一个复制文件的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyfile(fname, target_dir):\n",
    "    \"\"\"将文件复制到指定文件夹\"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    shutil.copy(fname, target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一个小数据集\n",
    "def reorg_train(data_dir, DF, ratio):\n",
    "    # 取多少的样本\n",
    "    n = int(len(DF)*ratio)\n",
    "    # 先写上cvs文件的列名\n",
    "    valid_list = [['ID', 'BoneAge', 'Male']]\n",
    "    # 在dataframe中随机遍历n个\n",
    "    for index, row in DF.sample(int(n)).iterrows():\n",
    "        # 获取当前文件路径\n",
    "        filename = os.path.join(data_dir, 'boneage-training-dataset', f\"{row['ID']}.png\")\n",
    "        # 复制到指定文件夹\n",
    "        copyfile(filename, os.path.join(data_dir, 'small-training-dataset'))\n",
    "        # 将该行数据添加至列表\n",
    "        valid_list.append([row['ID'], row['BoneAge'], row['Male']])\n",
    "    # 将list的内容写入csv文件\n",
    "    with open('./dataset/small-training-dataset.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in valid_list:\n",
    "            writer.writerow(row)\n",
    "    # # 读取验证集的csv，开始生成数据集，但是数据集必然很大\n",
    "    # for train_file in os.listdir(os.path.join(data_dir, 'boneage-training-dataset')):\n",
    "    #     ID = train_file.split('.')[0]\n",
    "    #     fname = os.path.join(data_dir, 'boneage-training-dataset', train_file)\n",
    "    return 0\n",
    "reorg_train(data_dir, training_csv, 0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义reorg_train_valid函数来将验证集从原始的训练集中拆分出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorg_train_valid(data_dir, DF, valid_ratio):\n",
    "    \"\"\"将验证集从原始的数据集中拆分出来\"\"\"\n",
    "    # 取多少的样本\n",
    "    n = int(len(DF)*valid_ratio)\n",
    "    # 先写上cvs文件的列名\n",
    "    valid_list = [['ID', 'BoneAge', 'Male']]\n",
    "    # 在dataframe中随机遍历n个\n",
    "    for index, row in DF.sample(int(n)).iterrows():\n",
    "        # 获取当前文件路径\n",
    "        filename = os.path.join(data_dir, 'boneage-training-dataset', f\"{row['ID']}.png\")\n",
    "        # 复制到指定文件夹\n",
    "        copyfile(filename, os.path.join(data_dir, 'boneage-valid-dataset'))\n",
    "        # 将该行数据添加至列表\n",
    "        valid_list.append([row['ID'], row['BoneAge'], row['Male']])\n",
    "    # 将list的内容写入csv文件\n",
    "    with open('./dataset/boneage-valid-dataset.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        for row in valid_list:\n",
    "            writer.writerow(row)\n",
    "    # # 读取验证集的csv，开始生成数据集，但是数据集必然很大\n",
    "    # for train_file in os.listdir(os.path.join(data_dir, 'boneage-training-dataset')):\n",
    "    #     ID = train_file.split('.')[0]\n",
    "    #     fname = os.path.join(data_dir, 'boneage-training-dataset', train_file)\n",
    "    return 0\n",
    "\n",
    "# reorg_train_valid(data_dir, training_csv, 0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_csv = pd.read_csv(os.path.join(data_dir, 'boneage-valid-dataset.csv'))\n",
    "test_csv = pd.read_csv(os.path.join(data_dir, 'boneage-test-dataset.csv'))\n",
    "small_csv = pd.read_csv(os.path.join(data_dir, 'small-training-dataset.csv'))\n",
    "# 数据预处理,数字标准化，非数字型转化\n",
    "boneage_mean = training_csv['BoneAge'].mean()\n",
    "boneage_std = training_csv['BoneAge'].std()\n",
    "training_csv['zscore'] = training_csv['BoneAge'].map(lambda x: (x - boneage_mean) / boneage_std)\n",
    "training_csv['Male']  = training_csv['Male'].astype('float32')\n",
    "training_csv['BoneAge'] = training_csv['BoneAge'].astype('float32')\n",
    "\n",
    "small_mean = small_csv['BoneAge'].mean()\n",
    "small_std = small_csv['BoneAge'].std()\n",
    "small_csv['zscore'] = small_csv['BoneAge'].map(lambda x: (x - small_mean) / small_std)\n",
    "small_csv['Male']  = small_csv['Male'].astype('float32')\n",
    "small_csv['BoneAge'] = small_csv['BoneAge'].astype('float32')\n",
    "\n",
    "valid_csv['Male'] = valid_csv['Male'].astype('float32')\n",
    "valid_csv['BoneAge'] = valid_csv['BoneAge'].astype('float32')\n",
    "valid_csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：图像增广\n",
    "使用图像增广来解决过拟合的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入用于图像增广的包\n",
    "from albumentations.augmentations.transforms import Lambda, RandomBrightnessContrast\n",
    "from albumentations.augmentations.geometric.transforms import ShiftScaleRotate, HorizontalFlip\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.crops.transforms import RandomResizedCrop\n",
    "from albumentations import Compose\n",
    "\n",
    "import cv2\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机删除一个图片上的像素，p为执行概率，scale擦除部分占据图片比例的范围，ratio擦除部分的长宽比范围\n",
    "randomErasing = transforms.RandomErasing(scale=(0.02, 0.08), ratio=(0.5, 2), p=0.8)\n",
    "\n",
    "def randomErase(image, **kwargs):\n",
    "    return randomErasing(image)\n",
    "\n",
    "\n",
    "x = torch.rand((512, 512, 3))\n",
    "\n",
    "randomErase(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化每个通道\n",
    "def sample_normalize(image, **kwargs):\n",
    "    image = image//255\n",
    "    channel = image.shape[2]\n",
    "    mean, std = image.reshape((-1, channel)).mean(axis = 0), image.reshape((-1, channel)).std(axis = 0)\n",
    "    return (image - mean)/(std + 1e-3)\n",
    "\n",
    "sample_normalize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 训练集的图像增广\n",
    "\n",
    "transform_train = Compose([\n",
    "    # 随机大小裁剪，512为调整后的图片大小，（0.5,1.0）为scale剪切的占比范围，概率p为0.5\n",
    "    RandomResizedCrop(512, 512, (0.5, 1.0), p = 0.5),\n",
    "    # ShiftScaleRotate操作：仿射变换，shift为平移，scale为缩放比率，rotate为旋转角度范围，border_mode用于外推法的标记，value即为padding_value，前者用到的，p为概率\n",
    "    ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=20, border_mode=cv2.BORDER_CONSTANT, value=0.0, p=0.8),\n",
    "    # 水平翻转\n",
    "    HorizontalFlip(p=0.5),\n",
    "    # 概率调整图片的对比度\n",
    "    RandomBrightnessContrast(p=0.8, contrast_limit=(-0.3, 0.2)),\n",
    "    # 标准化\n",
    "    Lambda(image=sample_normalize), \n",
    "    # 将图片转化为tensor类型\n",
    "    ToTensorV2(),\n",
    "    # 做随机擦除\n",
    "    Lambda(image=randomErase)\n",
    "])\n",
    "\n",
    "transform_train(image = convert_pic)['image']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遗留问题：验证集是从训练集里面提取出来的，然而并没有从训练集中抽离出来，所以是失败的，下面写一个k折交叉验证中获得验证集的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fole_data(k, i, X, y):\n",
    "    # 确保k合法\n",
    "    assert k>1\n",
    "    fold_size = X.shape[0] // i\n",
    "    X_train, y_train = 0, 0\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx, :]\n",
    "        if i == j :\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train == None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train ,X_valid, y_valid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 罚函数\n",
    "def L1_penalty(net, alpha):\n",
    "    loss = 0\n",
    "    for param in net.MLP.parameters():\n",
    "        loss += torch.sum(torch.abs(param))\n",
    "\n",
    "    return alpha*loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重写getitem函数，这样可以将这个类类似于DF\n",
    "class BAATrainDatasets(Dataset):\n",
    "    def __init__(self, df, file_path) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        num = int(row['ID'])\n",
    "        return (transform_train(image = read_iamge(self.file_path, f\"{num}.png\"))['image'], Tensor([row['Male']])), row['zscore']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "train_dataset = BAATrainDatasets(training_csv, os.path.join(data_dir, 'boneage-training-dataset'))\n",
    "small_dataset = BAATrainDatasets(small_csv, os.path.join(data_dir, 'small-training-dataset'))\n",
    "small_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "# loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "loss = nn.L1Loss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集的数据处理\n",
    "transform_valid = Compose([\n",
    "    Lambda(image=sample_normalize),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取验证集\n",
    "class BAAValidDatasets(Dataset):\n",
    "\n",
    "    def __init__(self, df, file_path) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        num = int(row['ID'])\n",
    "        return (transform_valid(image=read_iamge(self.file_path, f\"{num}.png\"))['image'], Tensor([row['Male']])), row['BoneAge']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据集\n",
    "train_dataset = BAATrainDatasets(training_csv, os.path.join(data_dir, 'boneage-training-dataset'))\n",
    "valid_dataset = BAAValidDatasets(valid_csv, os.path.join(data_dir, 'boneage-valid-dataset'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置采样器\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    train_dataset,\n",
    "    num_replicas=1,   # 检索参与复制的设备数\n",
    "    rank=0,  # 检索当前进程的复制序号，序数范围从 0 到 xrt_world_size()-1\n",
    "    shuffle=True)\n",
    "valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    valid_dataset,\n",
    "    num_replicas=1,\n",
    "    rank=0,\n",
    "    shuffle=True\n",
    ")\n",
    "small_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "    small_dataset,\n",
    "    num_replicas=1,   # 检索参与复制的设备数\n",
    "    rank=0,  # 检索当前进程的复制序号，序数范围从 0 到 xrt_world_size()-1\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=train_sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "valid_iter = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=valid_sampler,\n",
    "    drop_last=True\n",
    ")\n",
    "small_iter = torch.utils.data.DataLoader(\n",
    "    small_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=small_sampler,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取网络\n",
    "def get_net():\n",
    "    net = Ensemble()\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\n",
    "\n",
    "    Defined in :numref:`sec_softmax_scratch`\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = d2l.argmax(y_hat, axis=1)\n",
    "    cmp = d2l.astype(y_hat, y.dtype) == y\n",
    "    return float(d2l.reduce_sum(d2l.astype(cmp, y.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个在多个GPU上小批量训练的函数\n",
    "\n",
    "def train_batch(net, X, gender, y, loss, trainer):\n",
    "    # 将数据布置在设备上\n",
    "    # if isinstance(X, list):\n",
    "    #     # BERT中所需\n",
    "    #     # X = [x.to(devices[0]) for x in X]\n",
    "    #     X = [x.cuda() for x in X]\n",
    "    # else:\n",
    "    gender = gender.cuda()\n",
    "    X = X.cuda()\n",
    "    y = torch.reshape(y, [4, 1])\n",
    "    y = y.cuda()\n",
    "    # gender = gender.cuda()\n",
    "    # 开启微调\n",
    "    net.train()\n",
    "    # 将梯度清0\n",
    "    trainer.zero_grad()\n",
    "    X = X.float()\n",
    "    pred = net(X, gender)\n",
    "    pred = pred.squeeze()\n",
    "    l = loss(pred, y)\n",
    "    train_loss_sum = l + L1_penalty(net, 1e-5)\n",
    "    train_loss_sum.backward()\n",
    "    # l.sum().backward()\n",
    "    trainer.step()\n",
    "    # train_loss_sum = l.sum()\n",
    "    # print(f'loss_sum：{train_loss_sum}')\n",
    "    return train_loss_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\n",
    "\n",
    "    Defined in :numref:`sec_lenet`\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # No. of correct predictions, no. of predictions\n",
    "    metric = d2l.Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            valid_labels = y\n",
    "            valid_feature, valid_gender = X[0], X[1]\n",
    "            if isinstance(X, list):\n",
    "                # Required for BERT Fine-tuning (to be covered later)\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(valid_feature, valid_gender), valid_labels), d2l.size(valid_labels))\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "\n",
    "def train(net, train_iter, valid_iter, num_epoch, lr, wd, devices, lr_period, lr_decay):\n",
    "    # 定义Adam优化器\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "    # 每隔几个周期，学习率就衰减\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid acc')\n",
    "    # 可视化\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epoch], legend=legend)\n",
    "    # 在多个GPU上部署\n",
    "    net = net.to(device=try_gpu())\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        net.train()\n",
    "        # 可视化函数\n",
    "        metric = d2l.Accumulator(3)\n",
    "        for i, data in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            labels = data[1]\n",
    "            feature, gender = data[0]\n",
    "            l = train_batch(net, feature, gender, labels, loss, trainer)\n",
    "            metric.add(l, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches -1:\n",
    "                # 在animator中加入批次，以及平均损失和平均精确度\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1], None)\n",
    "                             )\n",
    "        # 若训练集不为空，则测试一下当前模型的精度，且将其在animator中显示\n",
    "        if valid_iter is not None:\n",
    "            for idx, data in enumerate(valid_iter):\n",
    "                # valid_labels = data[1]\n",
    "                # valid_feature, valid_gender = data[0]\n",
    "                valid_acc = evaluate_accuracy_gpu(net, valid_iter)\n",
    "                animator.add(epoch + 1 , (None, valid_acc))\n",
    "            \n",
    "        # 执行完一个小批次，使scheduler执行一下\n",
    "        scheduler.step()\n",
    "    measures = (f'train loss {metric[0] / metric[1]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid acc {valid_acc:.3f}'\n",
    "    print(measures + f'\\n{measures[1] * num_epoch / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices, num_epochs, lr, wd = try_gpu(), 10, 2e-4, 5e-4\n",
    "lr_period, lr_decay, net = 10, 0.5, get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, small_iter, valid_iter, num_epochs, lr, wd, devices, lr_period, lr_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0747f93ff6db21b2db2bf35ad4858dd0825b9c21797c41b4cc32097944ab3f10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
